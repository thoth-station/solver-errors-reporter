{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020578,
     "end_time": "2020-08-14T21:57:41.440758",
     "exception": false,
     "start_time": "2020-08-14T21:57:41.420180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pre-processing solver dataset and output clean dataset which is the input for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017433,
     "end_time": "2020-08-14T21:57:41.475778",
     "exception": false,
     "start_time": "2020-08-14T21:57:41.458345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Import Packages](#Import_packages)\n",
    "3. [Set environment variables to retrieve data from Ceph and map to dataframe](#retrieve_data_from_Ceph)\n",
    "4. [Load Error data from csv file created in the above step](#Load_data)\n",
    "5. [Split the error components from the log message](#split_error)\n",
    "6. [Example of error logs](#example)\n",
    "7. [Prepare the data for clustering](#prepare_data)\n",
    "8. [Cleaning clustering data](#clean_data)\n",
    "9. [Tokenization](#tokenization)\n",
    "10. [Save the cleaned data for clustering](#save_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021117,
     "end_time": "2020-08-14T21:57:41.516502",
     "exception": false,
     "start_time": "2020-08-14T21:57:41.495385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction  <a id='Introduction'></a>\n",
    "\n",
    "The purpose of this notebook is to preprocess solver data, i.e, extract error data from solver data, prepare data for clustering, clean and tokenize the clustering data and save the clean dataset for [ClusterError](./ClusterErrors.ipynb) notebook. \n",
    "\n",
    "\n",
    "Currently grafana metrics from the SolverResultsStore show over 700k results for solvers. This notebook filters for documents with solver errors. Each solver has around 20k - 30k results with solver error.\\\n",
    "Within the results, there are currently five different solvers:\n",
    "   - solver-rhel-8-py36\n",
    "   - solver-fedora-31-py38\n",
    "   - solver-fedora-31-py37\n",
    "   - solver-fedora-32-py38\n",
    "   - solver-fedora-32-py37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027434,
     "end_time": "2020-08-14T21:57:41.571821",
     "exception": false,
     "start_time": "2020-08-14T21:57:41.544387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import packages <a id='Import_packages'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T21:57:41.612389Z",
     "iopub.status.busy": "2020-08-14T21:57:41.611999Z",
     "iopub.status.idle": "2020-08-14T21:57:46.037591Z",
     "shell.execute_reply": "2020-08-14T21:57:46.037870Z"
    },
    "papermill": {
     "duration": 4.448738,
     "end_time": "2020-08-14T21:57:46.037975",
     "exception": false,
     "start_time": "2020-08-14T21:57:41.589237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "from thoth.lab import solver\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from string import punctuation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T21:57:46.087771Z",
     "iopub.status.busy": "2020-08-14T21:57:46.087290Z",
     "iopub.status.idle": "2020-08-14T21:57:46.088831Z",
     "shell.execute_reply": "2020-08-14T21:57:46.089101Z"
    },
    "papermill": {
     "duration": 0.029843,
     "end_time": "2020-08-14T21:57:46.089190",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.059347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022299,
     "end_time": "2020-08-14T21:57:46.132664",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.110365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set environment variables to retrieve data from Ceph and map to dataframe  <a id='retrieve_data_from_Ceph'></a>\n",
    "### DO NOT Run this everytime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02361,
     "end_time": "2020-08-14T21:57:46.178685",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.155075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### All the result in solver report are described below:\n",
    "\n",
    "- **environment,** information about the environment on which the package has being solved;\n",
    "- **environment_packages,** information about external packages installed on the environment;\n",
    "- **errors,** if the installation of a package was not succesfull there will be information stored for each package error;\n",
    "    - **details,**\n",
    "        - **command**,\n",
    "        - **message**,\n",
    "        - **return_code**,\n",
    "        - **stderr**,\n",
    "        - **stdout**,\n",
    "        - **timeout**,\n",
    "    - **index_url,** from where the package was download;\n",
    "    - **package_name;**\n",
    "    - **package_version;**\n",
    "    - **is_provided_package,** flag for storing package;\n",
    "    - **is_provided_package_version,** flag for storing package;\n",
    "    - **type,** error type;\n",
    "- **tree**, all the packages installed in the dependency tree and information about them;\n",
    "    - **dependencies**\n",
    "    - **metadata** of the package as taken from importlib_metadata;\n",
    "    - **index_url** from where the package was download;\n",
    "    - **package_name;**\n",
    "    - **package_version;**\n",
    "    - **sha256;**\n",
    "- **unparsed** if there are packages in the tree that could not be parsed;\n",
    "- **unresolved,** if there are packages in the tree that could not be solved;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T21:57:46.228090Z",
     "iopub.status.busy": "2020-08-14T21:57:46.227532Z",
     "iopub.status.idle": "2020-08-14T21:57:46.230872Z",
     "shell.execute_reply": "2020-08-14T21:57:46.230470Z"
    },
    "papermill": {
     "duration": 0.029458,
     "end_time": "2020-08-14T21:57:46.230968",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.201510",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "get_fresh_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T21:57:46.384934Z",
     "iopub.status.busy": "2020-08-14T21:57:46.384325Z",
     "iopub.status.idle": "2020-08-14T21:57:46.386136Z",
     "shell.execute_reply": "2020-08-14T21:57:46.386508Z"
    },
    "papermill": {
     "duration": 0.029704,
     "end_time": "2020-08-14T21:57:46.386687",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.356983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THOTH_S3_ENDPOINT_URL'] = 'https://s3.upshift.redhat.com/'\n",
    "os.environ['THOTH_CEPH_BUCKET'] = 'thoth'\n",
    "os.environ['THOTH_CEPH_HOST'] = 'https://s3.upshift.redhat.com/'\n",
    "os.environ['THOTH_CEPH_BUCKET_PREFIX'] = 'data/thoth'\n",
    "os.environ['THOTH_DEPLOYMENT_NAME'] = 'thoth-psi-stage'\n",
    "\n",
    "os.environ['THOTH_CEPH_KEY_ID'] = THOTH_CEPH_KEY_ID\n",
    "os.environ['THOTH_CEPH_SECRET_KEY'] = THOTH_CEPH_SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-08-14T21:57:46.439712Z",
     "iopub.status.busy": "2020-08-14T21:57:46.439087Z",
     "iopub.status.idle": "2020-08-14T21:57:46.441223Z",
     "shell.execute_reply": "2020-08-14T21:57:46.440826Z"
    },
    "papermill": {
     "duration": 0.034449,
     "end_time": "2020-08-14T21:57:46.441347",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.406898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if get_fresh_data:\n",
    "    # Connect to thoth storage\n",
    "    from thoth.storages import SolverResultsStore\n",
    "    store = SolverResultsStore(region=\"us-east-1\")\n",
    "    store.connect()\n",
    "    \n",
    "    solver_reports_extracted_data = []\n",
    "    solver_errors = []\n",
    "\n",
    "    # This block filters for documents with solver errors(Retrieve solver reports from Ceph and \n",
    "    # save only if it has \"errors\" in \n",
    "    for document_id in store.get_document_listing():\n",
    "        try:\n",
    "            solver_document = store.retrieve_document(document_id=document_id)\n",
    "            solver_report_extracted_data = solver.extract_data_from_solver_metadata(solver_document[\"metadata\"])\n",
    "            errors = solver.extract_errors_from_solver_result(solver_document[\"result\"][\"errors\"])   \n",
    "            for error in errors:\n",
    "                error['document_id'] = solver_report_extracted_data['document_id']\n",
    "                error['datetime'] = solver_report_extracted_data['datetime']\n",
    "                error['analyzer_version'] = solver_report_extracted_data['analyzer_version']\n",
    "                error['environment'] = solver_document[\"result\"][\"environment\"]\n",
    "                solver_errors.append(error)\n",
    "        except Exception as e:\n",
    "            print(document_id, e)\n",
    "            \n",
    "    solver_total = pd.DataFrame(solver_errors).reset_index()\n",
    "\n",
    "    solver_total['solver'] = solver_total['document_id'].str.rsplit(\"-\", n=1).str[0]\n",
    "    solver_total['solver'] = solver_total['solver'].replace('solver-rhel-8.0-py36', 'solver-rhel-8-py36')\n",
    "    \n",
    "    solver_total.to_csv('error_data.csv', index=False)\n",
    "    print(len(solver_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024889,
     "end_time": "2020-08-14T21:57:46.488418",
     "exception": false,
     "start_time": "2020-08-14T21:57:46.463529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Error data from csv file created in the above step <a id='Load_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2020-08-14T21:57:46.519362",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if get_fresh_data:\n",
    "    solver_total_errors_df = solver_total\n",
    "else:\n",
    "    solver_total_errors_df = pd.read_csv('error_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_total_errors_df['solver'] = solver_total_errors_df['solver'].replace('solver-rhel-8.0-py36', 'solver-rhel-8-py36')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_total_errors_df['solver'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_total_errors_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Split the error components from the log message <a id='split_error'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_log(log_messages):\n",
    "    log_messages = log_messages.split('\\n')\n",
    "    return log_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame()\n",
    "error_df['index'] = solver_total_errors_df['index']\n",
    "error_df['document_id'] = solver_total_errors_df['document_id']\n",
    "error_df['command'] = solver_total_errors_df['command']\n",
    "error_df['package_name'] = solver_total_errors_df['package_name']\n",
    "error_df['package_version'] = solver_total_errors_df['package_version']\n",
    "error_df['solver'] = solver_total_errors_df['solver']\n",
    "error_df['datetime'] = solver_total_errors_df['datetime']\n",
    "error_df['environment'] = solver_total_errors_df['environment']\n",
    "error_df['analyzer_version'] = solver_total_errors_df['analyzer_version']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['message'] = solver_total_errors_df['message']\n",
    "error_df['split_message']= solver_total_errors_df.apply(lambda row: split_log(row.message),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Example of error logs: <a id='example'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_total_errors_df['message'][999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['split_message'][999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_total_errors_df['message'][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['split_message'][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "solver_total_errors_df['message'][226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['split_message'][226]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_error_components(log_messages):\n",
    "    ids_with_different_log_pattern = []\n",
    "    Error_info, command_info, cwd, Complete_output, ERROR, specific_error, exception = {}, {}, {}, {}, {}, {}, {}\n",
    "    for idx, msg in enumerate(log_messages):\n",
    "        msg = msg.replace(\"Error:\\n\", \"Error:\")\n",
    "        sentences = [x.strip() for x in msg.split('\\n')]\n",
    "        for id, sent in enumerate(sentences):\n",
    "            if re.match(r\".*WARNING.*|.*warnings.*|.*except .*|^copying.*|^checking.*\", sent):\n",
    "                pass\n",
    "            elif re.match(r\"^command.*\", sent):\n",
    "                if idx in command_info.keys() and sent not in command_info[idx]:\n",
    "                    command_info[idx].append(sent)\n",
    "                else:\n",
    "                    command_info[idx] = [sent]\n",
    "            elif re.match(r\"^cwd.*\", sent):\n",
    "                if idx in cwd.keys() and sent not in cwd[idx]:\n",
    "                    cwd[idx].append(sent)\n",
    "                else:\n",
    "                    cwd[idx] = [sent]\n",
    "            elif re.match(r\"^Complete output.*\", sent):\n",
    "                number_of_lines = re.findall(r'\\d+', sent) \n",
    "                if idx in Complete_output.keys() and sent not in Complete_output[idx]:\n",
    "                    pass\n",
    "                else:\n",
    "                    Complete_output[idx] = sentences[id:id+int(number_of_lines[0])+1]\n",
    "                    for txt in Complete_output[idx]:\n",
    "                        if re.match(r\"^Exception.*\", txt):\n",
    "                            exception[idx] = [txt]\n",
    "            elif re.match(r\".*unable to execute.*\", sent):\n",
    "                specific_error[idx] = [sent]\n",
    "            elif re.compile(\"(\\w\\w*Error)\").findall(sent):\n",
    "                if re.match('^from .* import .*', sent):\n",
    "                    pass\n",
    "                elif idx in specific_error.keys():\n",
    "                    if sent in specific_error[idx]:\n",
    "                        pass\n",
    "                    if not re.match(r\".*unable to execute.*\", specific_error[idx][0]):\n",
    "                        specific_error[idx].extend([sent])\n",
    "                else:\n",
    "                    specific_error[idx] = [sent]\n",
    "            elif re.match(r\"^Error.*\", sent):\n",
    "                if idx not in specific_error.keys():\n",
    "                    specific_error[idx] = [sent]\n",
    "            elif re.match(r\"^ERROR: .*\", sent):\n",
    "                if idx in ERROR.keys() and sent not in ERROR[idx]:\n",
    "                    if not re.match(r\".*Failed.*\", ERROR[idx][0]):\n",
    "                        ERROR[idx].extend([sent])\n",
    "                else:\n",
    "                    ERROR[idx] = [sent]   \n",
    "            elif re.match(r\"^Command.*\", sent):\n",
    "                Error_info[idx] = sent\n",
    "    return Error_info, command_info, cwd, Complete_output, ERROR, specific_error, exception\n",
    "\n",
    "print(len(solver_total_errors_df['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "Error_info, command_info, cwd, Complete_output, ERROR, specific_error, exception = split_error_components(solver_total_errors_df['message'])\n",
    "print(len(Error_info), len(command_info), len(cwd), len(Complete_output), len(ERROR), len(specific_error), len(exception))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['Error_info']= error_df['index'].map(Error_info)\n",
    "error_df['command_info']= error_df['index'].map(command_info)\n",
    "error_df['cwd']= error_df['index'].map(cwd)\n",
    "error_df['Complete_output']= error_df['index'].map(Complete_output)\n",
    "error_df['ERROR']= error_df['index'].map(ERROR)\n",
    "error_df['Exception']= error_df['index'].map(exception)\n",
    "error_df['specific_error']= error_df['index'].map(specific_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Prepare the data for clustering <a id='prepare_data'></a>\n",
    "\n",
    "Look for data in the 'specific_error' column of the dataframe. If not present, look at 'ERROR' column or 'Error_info' column. If all the extracted columns are empty, take the error data from 'message' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustering_data = {}\n",
    "context_message = {}\n",
    "for idx, error in enumerate(error_df['specific_error']):\n",
    "    if type(error) != list:\n",
    "        #if type(error_df.iloc[idx]['Exception']) == list:\n",
    "            #clustering_data[idx] = error_df.iloc[idx]['Exception']\n",
    "        if pd.isnull(error) and  type(error_df.iloc[idx]['ERROR']) == list:\n",
    "            context_message[idx] = error_df.iloc[idx]['ERROR']\n",
    "            r = re.compile(\".*Failed.*|.*CUDA.*\")\n",
    "            match = list(filter(r.match, error_df.iloc[idx]['ERROR']))    \n",
    "            if len(error_df.iloc[idx]['ERROR']) == 1 or not match:\n",
    "                clustering_data[idx] = [error_df.iloc[idx]['ERROR'][0]]\n",
    "            else:\n",
    "                clustering_data[idx] = match\n",
    "        elif type(error_df.iloc[idx]['ERROR']) != list:\n",
    "            if pd.isnull(error_df.iloc[idx]['ERROR']):\n",
    "                context_message[idx] = error_df.iloc[idx]['Error_info']\n",
    "                clustering_data[idx] = [error_df.iloc[idx]['Error_info']]\n",
    "                if type(error_df.iloc[idx]['Error_info']) != list:\n",
    "                    if pd.isnull(error_df.iloc[idx]['Error_info']):\n",
    "                        context_message[idx] = error_df.iloc[idx]['message']\n",
    "                        clustering_data[idx] = [error_df.iloc[idx]['message']]        \n",
    "    else:\n",
    "        context_message[idx] = error_df.iloc[idx]['specific_error']     \n",
    "        # Extracting just the words with 'Error' as suffix to it and handling some edge cases. \n",
    "        r = re.compile(\"(\\w\\w*Error)\")     \n",
    "        string = ' '.join(error)\n",
    "        match = re.findall(r, string)\n",
    "        if match:\n",
    "            if len(match) == 1:\n",
    "                clustering_data[idx] = match\n",
    "            else:\n",
    "                if 'SyntaxError' in match:\n",
    "                    for err in match:\n",
    "                        if re.match(rf'.*{err},|.*{err}\\),', string):\n",
    "                            clustering_data[idx] = ['SyntaxError']\n",
    "                            break\n",
    "                    if idx not in clustering_data.keys():\n",
    "                        clustering_data[idx] = list(set(match))\n",
    "                elif {'OSError', 'EnvironmentError'} == set(match):\n",
    "                    clustering_data[idx] = ['OSError']\n",
    "                elif {'Error_GetLastError', 'AttributeError'} == set(match):\n",
    "                    clustering_data[idx] = ['AttributeError']\n",
    "                elif {'GrabNetworkError', 'GrabError', 'ModuleNotFoundError'} == set(match):\n",
    "                    clustering_data[idx] = ['ModuleNotFoundError']\n",
    "                elif {'ReadTimeoutError', 'EnvironmentError'} == set(match):\n",
    "                    clustering_data[idx] = ['ReadTimeoutError']\n",
    "                elif {'NewConnectionError', 'EnvironmentError'} == set(match):\n",
    "                    clustering_data[idx] = ['NewConnectionError']\n",
    "                else:\n",
    "                    clustering_data[idx] = list(set(match))\n",
    "        else:\n",
    "            clustering_data[idx] = error\n",
    "    if clustering_data[idx] == ['Command exited with non-zero status code (-9):']:\n",
    "        clustering_data[idx] = ['Error Info not Available']\n",
    "    string = ' '.join(clustering_data[idx])\n",
    "    if re.match(\".*make: .*|.*CMake .*|.*zError.*|.*SWIG_Python_TypeError.*|.*CatchlibLZMAError.*|.*PyExc_MemoryError.*|.*ViZDoomError.*|.*PyExc_KeyError.*|.*icsneoGetError.*|.*LapackSrcNotFoundError.*|.*PyFrame_FastToLocalsWithError.*\", string):\n",
    "    #if re.match(\".*zError.*|.*SWIG_Python_TypeError.*|.*CatchlibLZMAError.*|.*PyExc_KeyError.*|.*PyFrame_FastToLocalsWithError.*\", clustering_data[idx][0]):\n",
    "        clustering_data[idx] = error_df.iloc[idx]['ERROR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(clustering_data), len(context_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Cleaning clustering data <a id='clean_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_line_number = r'(at line[:]*\\s*\\d+)'\n",
    "_url = r'(http[s]|root|srm|file)*:(//|/)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "_filepath = \"(/[a-zA-Z\\./]*[\\s]?)\"\n",
    "path_regex = re.compile(r'(\\b\\w+://)\\S+(?=\\s)')\n",
    "file_regex = re.compile(r'(\\b[f|F]ile( exists)?:?\\s?)/\\S+(?=\\s)')\n",
    "py_regex = re.compile(r'/?\\b[-./_a-zA-Z0-9]+\\.py\\b')\n",
    "long_regex = re.compile(r'[-/_a-zA-Z0-9]{25,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_whitespaces(sentence):\n",
    "    return \" \".join(sentence.split())\n",
    "\n",
    "def substitute_path(string):\n",
    "    string = path_regex.sub(r'\\1', string)\n",
    "    string = py_regex.sub(r' ', string)\n",
    "    string = file_regex.sub(r'\\1', string)\n",
    "    string = long_regex.sub(r'', string)\n",
    "    return string\n",
    "\n",
    "def cleaner(log_messages):\n",
    "    clean_log = {}\n",
    "    for key in log_messages:\n",
    "        for item in log_messages[key]:\n",
    "            item = re.sub(_line_number, \"at line *\", item)\n",
    "            item = re.sub(_url, \" \", item)\n",
    "            item = re.sub(_filepath, \" \", item)\n",
    "            item = substitute_path(item)\n",
    "            if key in clean_log.keys() and item not in clean_log[key]:\n",
    "                clean_log[key] = clean_log[key] + ' ' + remove_whitespaces(item)\n",
    "            else:\n",
    "                clean_log[key] = remove_whitespaces(item)\n",
    "    return clean_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clustering_data = cleaner(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(clustering_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Tokenization <a id='tokenization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "stop = punctuation + \"``\" + \"''\" + '\"\"' + \"/\"\n",
    "table = str.maketrans(stop, ' '*len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(log_messages):\n",
    "    tokenized = []\n",
    "    for key, item in log_messages.items():\n",
    "        item = item.replace(' Error','Error').strip()\n",
    "        item = item.replace('Errno','').strip()\n",
    "        item = item.replace('Error :','Error:').strip()\n",
    "        item = item.replace('Exception:','').strip()\n",
    "        item = item.replace('Exception','').strip()\n",
    "        item = item.replace('\\'','').strip()\n",
    "        item = item.replace('=',' ').strip()\n",
    "        if error_df['package_name'][key] in item.split():\n",
    "            item = item.replace(error_df['package_name'][key],'').strip()\n",
    "        if \"interpreter:\" in item.split():\n",
    "            item = item.split(\"interpreter\",1)[1]\n",
    "        if \" Please\" in item:\n",
    "            item = item.split(\"Please\",1)[0]\n",
    "        if \"for\"  in item.split():\n",
    "            item = item.split(\"for\",1)[0] \n",
    "        if \"in\" in item.split():\n",
    "            if \"in an\" in item:\n",
    "                item = item.split(\"in an\",1)[1]\n",
    "            else:\n",
    "                item = item.split(\"in\",1)[0]\n",
    "        if \"on\" in item.split():\n",
    "            item = re.split(\" on \",item)[0] \n",
    "        if re.match(r\"^ERROR:.* |^Error:.* |^error:.*\", item):\n",
    "            item = re.split(\"^ERROR: |^error: |^Error: |^error:Error:\",item)[1].split('.')[0]\n",
    "        if item.strip() == '':\n",
    "            item = error_df.iloc[idx]['ERROR'][0].translate(table).strip()\n",
    "        if \"Command\" in item:\n",
    "            if \"ERROR:\" in item:\n",
    "                item = item.split(\"ERROR:\",1)[1] \n",
    "            if re.match(r\".*Command errored out with exit status.*\", item):\n",
    "                item = \"Check the logs\"\n",
    "        if \"not found\" in item:\n",
    "            words = [stemmer.stem(word) for word in item.split()] \n",
    "            item = ''\n",
    "            for word in words:\n",
    "                item += word[0].upper() + word[1:]\n",
    "            item += \"Error\"\n",
    "        if \":\" in item and item[0] != \":\":\n",
    "            item = item.split(\":\",1)[0] \n",
    "        if \",\" in item:\n",
    "            item = item.split(\",\",1)[0] \n",
    "        if \"JAVA_HOME\" in item:\n",
    "            item = 'JAVA HOME not set to a path containing the JDK'\n",
    "        if re.match(\"(\\w\\w* error)\", item):\n",
    "            item = item.split()[0] + \"Error\"\n",
    "        if \"XML\" in item:\n",
    "            item = \"cannot get pre-processor and compiler flags\"\n",
    "        item = item.translate(table).strip()\n",
    "        tokenized.append(TreebankWordTokenizer().tokenize(item))\n",
    "    cleaned_tokens = []\n",
    "    for id, row in enumerate(tokenized):\n",
    "        cleaned_tokens.append(list(filter(None, [i\n",
    "                                                 for i in row \n",
    "                                                 if i != error_df['package_name'][id]\n",
    "                                                 and not i.lower().isnumeric()])))\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_clustering_data = tokenization(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_clustering_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Save the cleaned data for clustering <a id='save_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['clustering_data'] = error_df['index'].map(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['tokenized_clustering_data'] = tokenized_clustering_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df['context_message'] = error_df['index'].map(context_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df.to_csv('error-clean-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(error_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "papermill": {
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "PreprocessSolverErrorData.ipynb",
   "output_path": "PreprocessSolverErrorData.ipynb",
   "parameters": {
    "THOTH_CEPH_KEY_ID": "DTG5R3EEWN9JBYJZH0DF",
    "THOTH_CEPH_SECRET_KEY": "pdcEGFERILlkRDGrCSxdIMaZVtNCOKvYP4Gf2b2x",
    "get_fresh_data": false
   },
   "start_time": "2020-08-14T21:57:40.630718",
   "version": "2.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
